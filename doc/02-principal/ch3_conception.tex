\chapter{Conception}
\label{ch:conception}

The project is focusing on optimizing the \acrlong{rkdp} algorithm
in a context of \acrshort{gpu} computing.
The conception of the project is finding all the possible adaptations that could
decrease the execution time of the algorithm.

\section{RKDP algorithm}
\label{ch:conception:rkdp}

This method has nine stages and is a fifth-order accurate method.
The first six stages are used to compute the end state, the seventh stage is used
in the seventh state and the last two stages are used to compute the error and the
middle state.

\section{Simulation}
\label{ch:conception:simulation}

\acrshort{rkdp} is a sequential function and like it is described in the chapter
\ref{ch:analyze:gpu:use-cases}, \acrshort{gpu}s or parallelization is not
so useful in this case.
This simulation has to aim to see how many times using multi-threads is improving
or not the execution time.

\subsection{Principle}
\label{ch:conception:simulation:principle}

In the implementation of the \acrshort{rkdp}, there are different types of code.
The first one is the code that must be executed sequentially and has no option
to be distributed on multiple threads.
The second one is the code that has only
the constraint that it has to be done before its first use.
The last one is a vector multiplication that has a specified place in the code
but can be executed in parallel.

The figure \ref{fig:conception:simulation:principle:code-type} show how the
fourth step is computed and which part of the code is from which type.
The red part is the dependent code, that has to be executed sequentially.
It is the compute of the intermediate state kx using the equation.
The blue part is the independent code that just has to be executed before its
first use. This is the computation of the coefficients with the step's size.
The green part is the vector multiplication that can be executed in parallel.
Each line has to be computed sequentially but the vector multiplication can be
done in parallel.

\image{0.5}{05-resources/img/conception/simulation-principle.excalidraw.png}
        {Example illustrating which part of the code is from which type}
        {fig:conception:simulation:principle:code-type}

The principle is to record when what type of code is executed to have an idea of
the proportion of time is spent in each type of code.
The figure \ref{fig:conception:simulation:principle:code} show the same step as
in the figure \ref{fig:conception:simulation:principle:code-type} but with the
logger to record the type of code and written in Python.

\begin{code}
    \captionof{listing}{Fourth step in python for the simulation}
    \label{code:conception:simulation:principle:code}
    \begin{minted}{python}
# Fourth step
logger(4, TaskType.DEPENDENT)
k4 = equation(*state)
state = beg_state
logger(4, TaskType.INDEPENDENT)
coef_a41 = step * a41
coef_a42 = step * a42
coef_a43 = step * a43
coef_a44 = step * a44
logger(4, TaskType.VECTOR_MULT)
axpy(coef_a41, k1, state)
axpy(coef_a42, k2, state)
axpy(coef_a43, k3, state)
axpy(coef_a44, k4, state)
    \end{minted}
\end{code}

The logger is an object that create an event with a type and a timestamp.
This object is used during the runtime but also after to display the results and
to compute the better thread workload using a \texttt{SchedulingTree} describe in
the next chapter~\ref{ch:conception:simulation:scheduling}.

\subsection{SchedulingTree}
\label{ch:conception:simulation:scheduling}

The \texttt{SchedulingTree} has been developed to compute the better start time
for a given task with the best wanted time.

To choose which task will be executed on which thread, we ask to every threads,
which are represented by a \texttt{SchedulingTree}, to know which thread could
start the thread with the best time.
When the best thread is found, the task is added to it.

The implementation is inspired from an interval tree.
The tasks are the leaf and every node has the same start than the left child and
the same end than the right child.
This allows the code to know if a task could potentially overlap and existing one.
To get the result, the leaf has to be read from the left to the right.

\subsection{Results}
\label{ch:conception:simulation:results}

Those results are obtaining with recording the data from a python script running
the \acrshort{rkdp} with the chaotic Lorenz attractor equation.

\image{1}{05-resources/img/conception/simulation-result.png}
        {Results of the simulation}
        {fig:conception:simulation:results}

The second graph of the figure \ref{fig:conception:simulation:results} show the
how the time is spent in each step.
This version is without some dead time that appears when recoding the events.
It is important to have this version because when we are simulating the
repartition on the threads, we are not taking into account the dead time.

The graph three and four are showing what each thread is doing.
We can see that pre-computing the coefficients with the three threads when the
first one is computing the equation seems to be a good idea.
Also, distributed the vector multiplication could help a lot to decrease the
execution time.

The last graph is like the two first one but with the new repartition of the
tasks.

To conclude, this simulation shows that we can expect an improvement of dividing
the time up by two.
However, this simulation is not taking the time due to the thread
synchronization and it is run in a totally different context than the real
implementation.

To implement this solution, the workload has to be divided between the threads.
According to the result, the best workload would be something like the figure
\ref{fig:conception:simulation:result:thread-workload}.
Where a controller is computing the intermediate state using the equation and the
other threads are computing the vector multiplication.

\image{0.5}{05-resources/img/conception/thread-workload.excalidraw.png}
        {Best theoretical repartition of the workload between the threads}
        {fig:conception:simulation:result:thread-workload}


\section{Shared memory management}
\label{ch:conception:shared}

The allocation of the shared memory is a part of some implementations of the
new \acrshort{rkdp}.
To use this memory, the developer must deal with constraints about the size and
the alignment of the data.

For each track, the shared must contains eight \texttt{OdeState} and one
\texttt{FieldStepperResult}. The \texttt{OdeState}s are the seven intermediate
states (kx) and the last one is the state that is updated during the
whole method (along\_state).
If the coefficients are pre-computed using multiple threads, they also have to be
stored in the shared memory.
The table \ref{tab:conception:shared-component} show the size of each objects
for a track.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Variable} & \textbf{Type} & \textbf{Size (bytes)} \\
        \hline
        ks & OdeState[7] & 45 * 7 = 315 \\
        \hline
        along\_state & OdeState & 45 \\
        \hline
        result & FieldStepperResult & 144 \\
        \hline
        (coef) & double[32] & 8 * 32 = 256 \\
        \hline
        \multicolumn{2}{|l|}{Total} & 528 (784) \\
        \hline
    \end{tabular}
    \captionof{table}{Objects stored in the shared memory for each track}
    \label{tab:conception:shared-component}
\end{table}

According to the table \ref{tab:conception:shared-component}, the block size
has a maximum dimension of 372 threads for the version without precomputed~\ref{eq:conception:shared:dimension-without-coef}
and 248 threads for the version with precomputed coefficients~\ref{eq:conception:shared:dimension-with-coef}.

\begin{equation}
    \frac{49152\frac{B}{Block} }{528 \frac{B}{Track} } * 4 \frac{Thread}{Track} = 93\frac{Track}{Block} * 4 \frac{Thread}{Track} = \textbf{372}\frac{Thread}{Block}
    \label{eq:conception:shared:dimension-without-coef}
\end{equation}

\begin{equation}
    \frac{49152\frac{B}{Block} }{784 \frac{B}{Track} } * 4 \frac{Thread}{Track} = 62\frac{Track}{Block} * 4 \frac{Thread}{Track} = \textbf{248}\frac{Thread}{Block}
    \label{eq:conception:shared:dimension-with-coef}
\end{equation}

However, the shared memory size is not the only constraint.
The shared memory can be allocated statically or dynamically.
In this project, the number of tracks is not known at compile time so it is
mandatory to use a dynamic allocation.
The easiest way to do is to have an array with the different type of objects and
use the id as index to access the data as shown in the figure \ref{fig:conception:shared:memory-organization}.

\image{1}{05-resources/img/conception/memory-management.excalidraw.png}
        {Shared memory management}
        {fig:conception:shared:memory-organization}

Assuming that the shared memory is a big array of void pointers and knowing the
number of tracks of the block it is possible to create aligned pointers to the
different objects.
The operator \texttt{[x]} on a pointer is returning the pointer to the address
\texttt{ptr + x * sizeof(type)}. So using \texttt{[n]} is giving the first
address of the next array.


\section{Number of iterations}
\label{ch:conception:iterations}

As shown in the chapter \ref{ch:analyze:atlas:implementation} and in the figure
\ref{fig:analyze:atlas:implementation:iteration-hit}, more than 80\% of the time
just one iteration.
However, this does not take into account the time spent to get these results.
For example, if a chord need 100 iterations, it has taken 100 times more time
than a chord that needs only one iteration.
To have a better view of the place that takes an iteration, the data have been
multiplied by the number of iterations.
The result if shown in the figure \ref{fig:conception:iterations:iteration-hit}.

\image{1}{05-resources/img/conception/time-iteration-hit.png}
        {Time than an iteration is hit multiplied by the number of iterations to
        have a idea of the time spent in each iteration.}
        {fig:conception:iterations:iteration-hit}

As we can see, the first iteration is still the most used.
However, there is a better repartition and we can't assume that doing only one
iteration during the test is representative of the real use of the algorithm.

Based on the unofficial rule of the 80/20, a good number of iterations
will be a number that represents 80\% of the time.
The figure \ref{fig:conception:iterations:cumulative-time} show that this number
is 40 iterations.

\image{0.5}{05-resources/img/conception/cumulative-time-iteration.png}
        {Cumulative percent of the time spent in the iterations with a marker at
        80\%.}
        {fig:conception:iterations:cumulative-time}